{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pytorch-datastream","text":"<p>Simple dataset to dataloader library for pytorch.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from datastream import Dataset, Datastream\n\ndataset = (\n    Dataset.from_subscriptable([1, 2, 3])\n    .map(lambda number: number + 1)\n)\n\nassert dataset[-1] == 4\n\ndata_loader = (\n    Datastream(dataset)\n    .data_loader(batch_size=16, n_batches_per_epoch=100)\n)\n\nassert len(next(iter(data_loader))) == 16\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Simple, readable dataset pipeline creation</li> <li>Built-in support for:</li> <li>Imbalanced datasets</li> <li>Oversampling / stratification</li> <li>Weighted sampling</li> <li>Easy conversion to PyTorch DataLoader</li> <li>Testable examples in documentation</li> <li>Type hints and Pydantic validation</li> <li>Clean, maintainable codebase</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install with poetry:</p> <pre><code>poetry add pytorch-datastream\n</code></pre> <p>Or with pip:</p> <pre><code>pip install pytorch-datastream\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Getting Started guide</li> <li>See the Dataset and Datastream API references</li> </ul>"},{"location":"dataset/","title":"Dataset","text":"<p>A <code>Dataset[T]</code> is a mapping that allows pipelining of functions in a readable syntax returning an example of type <code>T</code>.</p> <pre><code>from datastream import Dataset\n\nfruits_and_cost = (\n    ('apple', 5),\n    ('pear', 7),\n    ('banana', 14),\n    ('kiwi', 100),\n)\n\ndataset = (\n    Dataset.from_subscriptable(fruits_and_cost)\n    .starmap(lambda fruit, cost: (\n        fruit,\n        cost * 2,\n    ))\n)\n\nassert dataset[2] == ('banana', 28)\n</code></pre>"},{"location":"dataset/#class-methods","title":"Class Methods","text":""},{"location":"dataset/#from_subscriptable","title":"<code>from_subscriptable</code>","text":"<pre><code>from_subscriptable(data: Subscriptable[T]) -&gt; Dataset[T]\n</code></pre> <p>Create <code>Dataset</code> based on subscriptable i.e. implements <code>__getitem__</code> and <code>__len__</code>.</p>"},{"location":"dataset/#parameters","title":"Parameters","text":"<ul> <li><code>data</code>: Any object that implements <code>__getitem__</code> and <code>__len__</code></li> </ul>"},{"location":"dataset/#returns","title":"Returns","text":"<ul> <li>A new Dataset instance</li> </ul>"},{"location":"dataset/#notes","title":"Notes","text":"<p>Should only be used for simple examples as a <code>Dataset</code> created with this method does not support methods that require a source dataframe like <code>Dataset.split</code> and <code>Dataset.subset</code>.</p>"},{"location":"dataset/#from_dataframe","title":"<code>from_dataframe</code>","text":"<pre><code>from_dataframe(df: pd.DataFrame) -&gt; Dataset[pd.Series]\n</code></pre> <p>Create <code>Dataset</code> based on <code>pandas.DataFrame</code>.</p>"},{"location":"dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>df</code>: Source pandas DataFrame</li> </ul>"},{"location":"dataset/#returns_1","title":"Returns","text":"<ul> <li>A new Dataset instance where <code>__getitem__</code> returns a row from the dataframe</li> </ul>"},{"location":"dataset/#notes_1","title":"Notes","text":"<p><code>Dataset.map</code> should be given a function that takes a row from the dataframe as input.</p>"},{"location":"dataset/#examples","title":"Examples","text":"<pre><code>import pandas as pd\nfrom datastream import Dataset\n\ndataset = (\n    Dataset.from_dataframe(pd.DataFrame(dict(\n        number=[1, 2, 3]\n    )))\n    .map(lambda row: row['number'] + 1)\n)\n\nassert dataset[-1] == 4\n</code></pre>"},{"location":"dataset/#from_paths","title":"<code>from_paths</code>","text":"<pre><code>from_paths(paths: List[str], pattern: str) -&gt; Dataset[pd.Series]\n</code></pre> <p>Create <code>Dataset</code> from paths using regex pattern that extracts information from the path itself.</p>"},{"location":"dataset/#parameters_2","title":"Parameters","text":"<ul> <li><code>paths</code>: List of file paths</li> <li><code>pattern</code>: Regex pattern with named groups to extract information from paths</li> </ul>"},{"location":"dataset/#returns_2","title":"Returns","text":"<ul> <li>A new Dataset instance where <code>__getitem__</code> returns a row from the generated dataframe</li> </ul>"},{"location":"dataset/#notes_2","title":"Notes","text":"<p><code>Dataset.map</code> should be given a function that takes a row from the dataframe as input.</p>"},{"location":"dataset/#examples_1","title":"Examples","text":"<pre><code>from datastream import Dataset\n\nimage_paths = [\"dataset/damage/1.png\"]\ndataset = (\n    Dataset.from_paths(image_paths, pattern=r\".*/(?P&lt;class_name&gt;\\w+)/(?P&lt;index&gt;\\d+).png\")\n    .map(lambda row: row[\"class_name\"])\n)\n\nassert dataset[-1] == 'damage'\n</code></pre>"},{"location":"dataset/#instance-methods","title":"Instance Methods","text":""},{"location":"dataset/#map","title":"<code>map</code>","text":"<pre><code>map(self, function: Callable[[T], U]) -&gt; Dataset[U]\n</code></pre> <p>Creates a new dataset with the function added to the dataset pipeline.</p>"},{"location":"dataset/#parameters_3","title":"Parameters","text":"<ul> <li><code>function</code>: Function to apply to each example</li> </ul>"},{"location":"dataset/#returns_3","title":"Returns","text":"<ul> <li>A new Dataset with the mapping function added to the pipeline</li> </ul>"},{"location":"dataset/#examples_2","title":"Examples","text":"<pre><code>from datastream import Dataset\n\ndataset = (\n    Dataset.from_subscriptable([1, 2, 3])\n    .map(lambda number: number + 1)\n)\n\nassert dataset[-1] == 4\n</code></pre>"},{"location":"dataset/#starmap","title":"<code>starmap</code>","text":"<pre><code>starmap(self, function: Callable[..., U]) -&gt; Dataset[U]\n</code></pre> <p>Creates a new dataset with the function added to the dataset pipeline.</p>"},{"location":"dataset/#parameters_4","title":"Parameters","text":"<ul> <li><code>function</code>: Function that accepts multiple arguments unpacked from the pipeline output</li> </ul>"},{"location":"dataset/#returns_4","title":"Returns","text":"<ul> <li>A new Dataset with the mapping function added to the pipeline</li> </ul>"},{"location":"dataset/#notes_3","title":"Notes","text":"<p>The dataset's pipeline should return an iterable that will be expanded as arguments to the mapped function.</p>"},{"location":"dataset/#examples_3","title":"Examples","text":"<pre><code>from datastream import Dataset\n\ndataset = (\n    Dataset.from_subscriptable([1, 2, 3])\n    .map(lambda number: (number, number + 1))\n    .starmap(lambda number, plus_one: number + plus_one)\n)\n\nassert dataset[-1] == 7\n</code></pre>"},{"location":"dataset/#subset","title":"<code>subset</code>","text":"<pre><code>subset(self, function: Callable[[pd.DataFrame], pd.Series]) -&gt; Dataset[T]\n</code></pre> <p>Select a subset of the dataset using a function that receives the source dataframe as input.</p>"},{"location":"dataset/#parameters_5","title":"Parameters","text":"<ul> <li><code>function</code>: Function that takes a DataFrame and returns a boolean mask</li> </ul>"},{"location":"dataset/#returns_5","title":"Returns","text":"<ul> <li>A new Dataset containing only the selected examples</li> </ul>"},{"location":"dataset/#notes_4","title":"Notes","text":"<p>This function can still be called after multiple operations such as mapping functions as it uses the source dataframe.</p>"},{"location":"dataset/#examples_4","title":"Examples","text":"<pre><code>import pandas as pd\nfrom datastream import Dataset\n\ndataset = (\n    Dataset.from_dataframe(pd.DataFrame(dict(\n        number=[1, 2, 3]\n    )))\n    .map(lambda row: row['number'])\n    .subset(lambda dataframe: dataframe['number'] &lt;= 2)\n)\n\nassert dataset[-1] == 2\n</code></pre>"},{"location":"dataset/#split","title":"<code>split</code>","text":"<pre><code>split(\n    self,\n    key_column: str,\n    proportions: Dict[str, float],\n    stratify_column: Optional[str] = None,\n    filepath: Optional[str] = None,\n    seed: Optional[int] = None,\n) -&gt; Dict[str, Dataset[T]]\n</code></pre> <p>Split dataset into multiple parts.</p>"},{"location":"dataset/#parameters_6","title":"Parameters","text":"<ul> <li><code>key_column</code>: Column to use as unique identifier for examples</li> <li><code>proportions</code>: Dictionary mapping split names to proportions</li> <li><code>stratify_column</code>: Optional column to use for stratification</li> <li><code>filepath</code>: Optional path to save/load split configuration</li> <li><code>seed</code>: Optional random seed for reproducibility</li> </ul>"},{"location":"dataset/#returns_6","title":"Returns","text":"<ul> <li>Dictionary mapping split names to Dataset instances</li> </ul>"},{"location":"dataset/#notes_5","title":"Notes","text":"<p>Optionally you can stratify on a column in the source dataframe or save the split to a json file. If you are sure that the split strategy will not change then you can safely use a seed instead of a filepath.</p> <p>Saved splits can continue from the old split and handle:</p> <ul> <li>New examples</li> <li>Changing test size</li> <li>Adapt after removing examples from dataset</li> <li>Adapt to new stratification</li> </ul>"},{"location":"dataset/#examples_5","title":"Examples","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom datastream import Dataset\n\nsplit_datasets = (\n    Dataset.from_dataframe(pd.DataFrame(dict(\n        index=np.arange(100),\n        number=np.arange(100),\n    )))\n    .map(lambda row: row['number'])\n    .split(\n        key_column='index',\n        proportions=dict(train=0.8, test=0.2),\n        seed=700,\n    )\n)\nassert len(split_datasets['train']) == 80\nassert split_datasets['test'][0] == 3\n</code></pre>"},{"location":"dataset/#zip_index","title":"<code>zip_index</code>","text":"<pre><code>zip_index(self) -&gt; Dataset[Tuple[T, int]]\n</code></pre> <p>Zip the output with its underlying Dataset index.</p>"},{"location":"dataset/#returns_7","title":"Returns","text":"<ul> <li>A new Dataset where each example is a tuple of <code>(output, index)</code></li> </ul>"},{"location":"dataset/#examples_6","title":"Examples","text":"<pre><code>from datastream import Dataset\n\ndataset = Dataset.from_subscriptable([4, 5, 6]).zip_index()\nassert dataset[0] == (4, 0)\n</code></pre>"},{"location":"dataset/#cache","title":"<code>cache</code>","text":"<pre><code>cache(self, key_column: str) -&gt; Dataset[T]\n</code></pre> <p>Cache intermediate step in-memory based on key column.</p>"},{"location":"dataset/#parameters_7","title":"Parameters","text":"<ul> <li><code>key_column</code>: Column to use as cache key</li> </ul>"},{"location":"dataset/#returns_8","title":"Returns","text":"<ul> <li>A new Dataset with caching enabled</li> </ul>"},{"location":"dataset/#examples_7","title":"Examples","text":"<pre><code>import pandas as pd\nfrom datastream import Dataset\n\ndf = pd.DataFrame({'key': ['a', 'b'], 'value': [1, 2]})\ndataset = Dataset.from_dataframe(df).cache('key')\nassert dataset[0]['value'] == 1\n</code></pre>"},{"location":"dataset/#concat","title":"<code>concat</code>","text":"<pre><code>concat(datasets: List[Dataset[T]]) -&gt; Dataset[T]\n</code></pre> <p>Concatenate multiple datasets together.</p>"},{"location":"dataset/#parameters_8","title":"Parameters","text":"<ul> <li><code>datasets</code>: List of datasets to concatenate</li> </ul>"},{"location":"dataset/#returns_9","title":"Returns","text":"<ul> <li>A new Dataset combining all input datasets</li> </ul>"},{"location":"dataset/#notes_6","title":"Notes","text":"<p>Consider using <code>Datastream.merge</code> if you have multiple data sources instead as it allows you to control the number of samples from each source in the training batches.</p>"},{"location":"dataset/#examples_8","title":"Examples","text":"<pre><code>from datastream import Dataset\n\ndataset1 = Dataset.from_subscriptable([1, 2])\ndataset2 = Dataset.from_subscriptable([3, 4])\ncombined = Dataset.concat([dataset1, dataset2])\nassert len(combined) == 4\nassert combined[2] == 3\n</code></pre>"},{"location":"dataset/#combine","title":"<code>combine</code>","text":"<pre><code>combine(datasets: List[Dataset]) -&gt; Dataset[Tuple]\n</code></pre> <p>Zip multiple datasets together so that all combinations of examples are possible.</p>"},{"location":"dataset/#parameters_9","title":"Parameters","text":"<ul> <li><code>datasets</code>: List of datasets to combine</li> </ul>"},{"location":"dataset/#returns_10","title":"Returns","text":"<ul> <li>A new Dataset yielding tuples of all possible combinations</li> </ul>"},{"location":"dataset/#notes_7","title":"Notes","text":"<p>Creates tuples like <code>(example1, example2, ...)</code> for all possible combinations (i.e. the cartesian product).</p>"},{"location":"dataset/#examples_9","title":"Examples","text":"<pre><code>from datastream import Dataset\n\ndataset1 = Dataset.from_subscriptable([1, 2])\ndataset2 = Dataset.from_subscriptable([3, 4])\ncombined = Dataset.combine([dataset1, dataset2])\nassert len(combined) == 4  # 2 * 2 = 4 combinations\nassert combined[0] == (1, 3)  # First combination\n</code></pre>"},{"location":"datastream/","title":"Datastream","text":"<p>A <code>Datastream[T]</code> combines a <code>Dataset[T]</code> and a sampler into a stream of examples.</p> <p>By default, samples are drawn without replacement until the dataset is exhausted. The sampling behavior can be modified using <code>sample_proportion</code>.</p>"},{"location":"datastream/#basic-usage","title":"Basic Usage","text":""},{"location":"datastream/#examples","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\n# Create a simple dataset\ndataset = Dataset.from_subscriptable([1, 2, 3])\n\n# Create a datastream with batching\ndata_loader = (\n    Datastream(dataset)\n    .data_loader(batch_size=2)\n)\n\n# First batch should have 2 items\nbatch = next(iter(data_loader))\nassert len(batch) == 2\n</code></pre>"},{"location":"datastream/#constructor","title":"Constructor","text":""},{"location":"datastream/#datastream_1","title":"<code>Datastream</code>","text":"<pre><code>Datastream(dataset: Dataset[T], sampler: Optional[torch.utils.data.Sampler] = None) -&gt; Datastream[T]\n</code></pre> <p>Create a new datastream from a dataset and optional sampler.</p>"},{"location":"datastream/#parameters","title":"Parameters","text":"<ul> <li><code>dataset</code>: The source dataset to stream from</li> <li><code>sampler</code>: Optional sampler to use. If None, a StandardSampler will be used</li> </ul>"},{"location":"datastream/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code>: If dataset is empty</li> </ul>"},{"location":"datastream/#data-loading-methods","title":"Data Loading Methods","text":""},{"location":"datastream/#data_loader","title":"<code>data_loader</code>","text":"<pre><code>data_loader(self, n_batches_per_epoch: Optional[int] = None, **kwargs) -&gt; torch.utils.data.DataLoader\n</code></pre> <p>Get a PyTorch DataLoader for use in training pipeline.</p>"},{"location":"datastream/#parameters_1","title":"Parameters","text":"<ul> <li><code>n_batches_per_epoch</code>: Optional number of batches per epoch. If provided, overrides the underlying length of the dataset</li> <li><code>**kwargs</code>: Additional arguments passed to PyTorch DataLoader</li> </ul>"},{"location":"datastream/#returns","title":"Returns","text":"<ul> <li>A PyTorch DataLoader instance</li> </ul>"},{"location":"datastream/#notes","title":"Notes","text":"<p>If <code>n_batches_per_epoch</code> is set and the epoch ends before the full dataset has been processed, it will continue from the same point in the next epoch.</p> <p>This is particularly useful when:</p> <ul> <li>Training on very large datasets where you want fixed-size epochs</li> <li>Using weighted sampling where you want to ensure all classes are seen equally</li> <li>Doing curriculum learning where you want to control exactly how many samples are seen</li> </ul>"},{"location":"datastream/#examples_1","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\ndata_loader = (\n    Datastream(Dataset.from_subscriptable([5, 5, 5]))\n    .data_loader(batch_size=2, n_batches_per_epoch=3)\n)\nbatches = list(data_loader)\nassert len(batches) == 3  # Always get exactly 3 batches\nassert len(batches[0]) == 2  # Each batch has size 2\n</code></pre>"},{"location":"datastream/#sampling-methods","title":"Sampling Methods","text":""},{"location":"datastream/#sample_proportion","title":"<code>sample_proportion</code>","text":"<pre><code>sample_proportion(self, proportion: float) -&gt; Datastream[T]\n</code></pre> <p>Create new Datastream with changed sampling proportion.</p>"},{"location":"datastream/#parameters_2","title":"Parameters","text":"<ul> <li><code>proportion</code>: The proportion of the dataset to sample before allowing replacement</li> </ul>"},{"location":"datastream/#returns_1","title":"Returns","text":"<ul> <li>A new Datastream with modified sampling behavior</li> </ul>"},{"location":"datastream/#notes_1","title":"Notes","text":"<p>This changes the number of drawn samples before restarting sampling with new weights and allowing sample replacement.</p> <p>It is important to set this if you are using sample weights because the default is to sample without replacement with proportion 1.0, which will cause the weighting scheme to only affect the order in which the samples are drawn.</p>"},{"location":"datastream/#examples_2","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\n# Create a datastream that will draw half the dataset before allowing replacement\ndatastream = (\n    Datastream(Dataset.from_subscriptable([1, 2, 3, 4]))\n    .sample_proportion(0.5)  # Draw 2 samples before replacement\n)\n\n# Sample size is still the full dataset length\nassert len(list(datastream)) == len(datastream)\n\n# But after 2 samples, items can be repeated\nsamples = []\nfor _ in range(4):\n    samples.extend(list(datastream))\nassert len(set(samples)) &lt; len(samples)  # Some samples are repeated\n</code></pre>"},{"location":"datastream/#take","title":"<code>take</code>","text":"<pre><code>take(self, n_samples: PositiveInt) -&gt; Datastream[T]\n</code></pre> <p>Create new Datastream that draws a fixed number of samples.</p>"},{"location":"datastream/#parameters_3","title":"Parameters","text":"<ul> <li><code>n_samples</code>: Number of samples to draw before allowing replacement</li> </ul>"},{"location":"datastream/#returns_2","title":"Returns","text":"<ul> <li>A new Datastream with modified sampling behavior</li> </ul>"},{"location":"datastream/#notes_2","title":"Notes","text":"<p>Like <code>sample_proportion</code> but specify the number of samples instead of a proportion.</p>"},{"location":"datastream/#examples_3","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\ndatastream = (\n    Datastream(Dataset.from_subscriptable([1, 2, 3, 4, 5]))\n    .take(2)  # Draw exactly 2 samples before allowing replacement\n)\nassert len(list(datastream)) == 2\n</code></pre>"},{"location":"datastream/#weight-management-methods","title":"Weight Management Methods","text":""},{"location":"datastream/#weight","title":"<code>weight</code>","text":"<pre><code>weight(self, index: int) -&gt; float\n</code></pre> <p>Get sample weight for specific example.</p>"},{"location":"datastream/#parameters_4","title":"Parameters","text":"<ul> <li><code>index</code>: Index of the example to get weight for</li> </ul>"},{"location":"datastream/#returns_3","title":"Returns","text":"<ul> <li>The weight of the example at the given index</li> </ul>"},{"location":"datastream/#notes_3","title":"Notes","text":"<p>Weights affect the probability of sampling each example.</p>"},{"location":"datastream/#examples_4","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\ndatastream = Datastream(Dataset.from_subscriptable([1, 2, 3]))\nassert datastream.weight(0) == 1.0  # Default weight is 1.0\n</code></pre>"},{"location":"datastream/#update_weights_","title":"<code>update_weights_</code>","text":"<pre><code>update_weights_(self, function: Callable[[np.array], np.array]) -&gt; None\n</code></pre> <p>Update all sample weights by function in-place.</p>"},{"location":"datastream/#parameters_5","title":"Parameters","text":"<ul> <li><code>function</code>: Function that takes array of weights and returns modified weights</li> </ul>"},{"location":"datastream/#notes_4","title":"Notes","text":"<p>This is useful for implementing importance sampling or curriculum learning strategies.</p>"},{"location":"datastream/#examples_5","title":"Examples","text":"<pre><code>import numpy as np\nfrom datastream import Dataset, Datastream\n\n# Create a datastream where we'll downweight all samples\ndatastream = Datastream(Dataset.from_subscriptable([1, 2, 3]))\ndatastream.update_weights_(lambda weights: weights * 0.5)\nassert datastream.weight(0) == 0.5\n</code></pre>"},{"location":"datastream/#update_example_weight_","title":"<code>update_example_weight_</code>","text":"<pre><code>update_example_weight_(self, weight: Union[List, float], index: int) -&gt; None\n</code></pre> <p>Update sample weight for specific example in-place.</p>"},{"location":"datastream/#parameters_6","title":"Parameters","text":"<ul> <li><code>weight</code>: New weight value(s) for the example</li> <li><code>index</code>: Index of the example to update</li> </ul>"},{"location":"datastream/#notes_5","title":"Notes","text":"<p>This is useful when you want to adjust the sampling probability of individual examples, for instance based on model performance.</p>"},{"location":"datastream/#examples_6","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\ndatastream = Datastream(Dataset.from_subscriptable([1, 2, 3]))\ndatastream.update_example_weight_(0.5, index=0)  # Make first example half as likely\nassert datastream.weight(0) == 0.5\n</code></pre>"},{"location":"datastream/#multi_sample","title":"<code>multi_sample</code>","text":"<pre><code>multi_sample(self, n: int) -&gt; Datastream[T]\n</code></pre> <p>Split datastream into clones with different sample weights and merge them.</p>"},{"location":"datastream/#parameters_7","title":"Parameters","text":"<ul> <li><code>n</code>: Number of weight clones to create</li> </ul>"},{"location":"datastream/#returns_4","title":"Returns","text":"<ul> <li>A new Datastream with multiple weight sets</li> </ul>"},{"location":"datastream/#notes_6","title":"Notes","text":"<p>The weights when accessed will be a sequence of multiple weights. This allows sample strategies where you for example stratify based on the model's predictions. A common use case is handling multi-label classification where you want to ensure good coverage of all classes.</p>"},{"location":"datastream/#examples_7","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\nn_classes = 3\ndatastream = (\n    Datastream(Dataset.from_subscriptable([1, 2, 3]))\n    .zip_index()\n    .multi_sample(n_classes)\n    .sample_proportion(0.5)\n)\n\n# Each example now has n_classes weights that can be adjusted independently\nweights = [datastream.weight(0) for _ in range(n_classes)]\nassert len(weights) == n_classes\n</code></pre>"},{"location":"datastream/#static-methods","title":"Static Methods","text":""},{"location":"datastream/#merge","title":"<code>merge</code>","text":"<pre><code>merge(datastreams_and_ns: Tuple[Union[Datastream[T], Tuple[Datastream[T], int]], ...]) -&gt; Datastream[T]\n</code></pre> <p>Creates a merged datastream where samples are drawn one at a time from each underlying datastream.</p>"},{"location":"datastream/#parameters_8","title":"Parameters","text":"<ul> <li><code>datastreams_and_ns</code>: List of datastreams or tuples of (datastream, n_samples)</li> </ul>"},{"location":"datastream/#returns_5","title":"Returns","text":"<ul> <li>A new merged Datastream</li> </ul>"},{"location":"datastream/#notes_7","title":"Notes","text":"<p>Also known as \"interleave\". Optionally you can define the number of drawn samples per Datastream.</p> <p>This is useful when you want to:</p> <ul> <li>Combine multiple data sources with different sampling rates</li> <li>Implement curriculum learning by controlling how often each type of example is seen</li> <li>Balance between different tasks in multi-task learning</li> </ul>"},{"location":"datastream/#examples_8","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\ndatastream1 = Datastream(Dataset.from_subscriptable([1, 1]))  # Task 1\ndatastream2 = Datastream(Dataset.from_subscriptable([2, 2]))  # Task 2\ndatastream3 = Datastream(Dataset.from_subscriptable([3, 3, 3, 3]))  # Task 3\n\n# Draw more samples from task 3 (might be harder to learn)\nmerged = Datastream.merge([\n    (datastream1, 1),  # Draw 1 sample at a time from task 1\n    (datastream2, 1),  # Draw 1 sample at a time from task 2\n    (datastream3, 2),  # Draw 2 samples at a time from task 3\n])\n\nsamples = list(merged)\nassert samples == [1, 2, 3, 3, 1, 2, 3, 3]  # Task 3 appears twice as often\n</code></pre>"},{"location":"datastream/#zip","title":"<code>zip</code>","text":"<pre><code>zip(datastreams: List[Datastream]) -&gt; Datastream[Tuple]\n</code></pre> <p>Zip multiple datastreams together so that samples are drawn independently.</p>"},{"location":"datastream/#parameters_9","title":"Parameters","text":"<ul> <li><code>datastreams</code>: List of datastreams to zip together</li> </ul>"},{"location":"datastream/#returns_6","title":"Returns","text":"<ul> <li>A new zipped Datastream that yields tuples</li> </ul>"},{"location":"datastream/#notes_8","title":"Notes","text":"<p>Samples are drawn independently from each underlying datastream, creating tuples like <code>(example1, example2, ...)</code>. This is different from <code>Dataset.combine</code>, which creates all possible combinations (cartesian product) of examples.</p> <p>This is particularly useful for:</p> <ul> <li>Creating paired samples for contrastive learning</li> <li>Implementing data augmentation strategies</li> <li>Combining different types of inputs</li> </ul>"},{"location":"datastream/#examples_9","title":"Examples","text":"<pre><code>from datastream import Dataset, Datastream\n\n# Create two streams: one for images, one for labels\ndatastream1 = Datastream(Dataset.from_subscriptable([1, 2]))  # e.g., image IDs\ndatastream2 = Datastream(Dataset.from_subscriptable([3, 4]))  # e.g., augmentation params\n\n# Get samples drawn independently from each datastream\nzipped = Datastream.zip([datastream1, datastream2])\nsamples = list(zipped)\nprint(\"Samples:\", samples)  # Debug output\nprint(\"Length:\", len(samples))  # Debug output\nprint(\"Expected length:\", max(len(datastream1.dataset), len(datastream2.dataset)))  # Debug output\nassert len(samples) == 2  # Independent samples: (1,3), (2,4)\n\n# For comparison, Dataset.combine creates all possible combinations\ncombined = Dataset.combine([datastream1.dataset, datastream2.dataset])\ncombined_samples = list(combined)\nprint(\"Combined samples:\", combined_samples)  # Debug output\nprint(\"Combined length:\", len(combined_samples))  # Debug output\nprint(\"Expected combined length:\", len(datastream1.dataset) * len(datastream2.dataset))  # Debug output\nassert len(combined_samples) == 4  # All combinations: (1,3), (1,4), (2,3), (2,4)\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install pytorch-datastream\n</code></pre>"},{"location":"getting-started/#usage","title":"Usage","text":""},{"location":"getting-started/#dataset","title":"Dataset","text":"<p>A <code>Dataset[T]</code> is a mapping that allows pipelining of functions in a readable syntax returning an example of type <code>T</code>.</p> <pre><code>from datastream import Dataset\n\nfruits_and_cost = (\n    ('apple', 5),\n    ('pear', 7),\n    ('banana', 14),\n    ('kiwi', 100),\n)\n\ndataset = (\n    Dataset.from_subscriptable(fruits_and_cost)\n    .starmap(lambda fruit, cost: (\n        fruit,\n        cost * 2,\n    ))\n)\n\nassert dataset[2] == ('banana', 28)\n</code></pre>"},{"location":"getting-started/#datastream","title":"Datastream","text":"<p>A <code>Datastream[T]</code> is an iterable that yields batches of type <code>T</code> from one or more datasets.</p> <pre><code>import numpy as np\nfrom datastream import Dataset, Datastream\n\ndataset = Dataset.from_subscriptable([1, 2, 3, 4])\ndatastream = Datastream(dataset)\n\nfor batch in datastream.data_loader(batch_size=2):\n    assert len(batch) == 2\n</code></pre>"},{"location":"getting-started/#merge","title":"Merge","text":"<p>Merge multiple datasets into a single datastream. The proportion of samples from each dataset in a batch can be controlled by passing tuples of <code>(datastream, proportion)</code>.</p> <pre><code>import numpy as np\nfrom datastream import Dataset, Datastream\n\ndataset1 = Dataset.from_subscriptable([1, 2, 3, 4])\ndataset2 = Dataset.from_subscriptable([5, 6, 7, 8])\n\ndatastream = Datastream.merge([\n    (Datastream(dataset1), 1),\n    (Datastream(dataset2), 1),\n])\n\nfor batch in datastream.data_loader(batch_size=2):\n    assert len(batch) == 2\n</code></pre>"}]}